{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Number from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there might be numbers inside that text like    '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_num (text):\n",
    "    result = re.sub(r'\\d+',\"\",text)\n",
    "    return result \n",
    "\n",
    "input_s  = \"there might be 5numbers inside that text like 12 2 3 \"\n",
    "remove_num(input_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Numbers into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there might be six hundred and forty-five 5numbers inside that text like one two three'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inflect \n",
    "\n",
    "q = inflect.engine()\n",
    "\n",
    "def convert_num(text):\n",
    "    \n",
    "    #split string into list of texts \n",
    "    temp_string = text.split()\n",
    "    \n",
    "    #intialize list \n",
    "    new_str = []\n",
    "    \n",
    "    for word in temp_string:\n",
    "        if word.isdigit():\n",
    "            temp = q.number_to_words(word)\n",
    "            new_str.append(temp)\n",
    "            \n",
    "        else:\n",
    "            new_str.append(word)\n",
    "            \n",
    "    return ' '.join(new_str)\n",
    "\n",
    "input_s  = \"there might be 645 5numbers inside that text like 1 2 3 \"\n",
    "convert_num(input_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation\n",
    "\n",
    "if we have comma, question mark, exclaimation marks that all will be removed by this process\n",
    "\n",
    "#### Note: \n",
    "you can even remove numbers, asci values whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you exicted After a week we will be in Pakistan for sure'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rem_punctuation(text):\n",
    "    translator = str.maketrans('','',string.punctuation )\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str = \"Hey, Are you exicted?? After a week, we will be in Pakistan for sure!!!\"\n",
    "rem_punctuation(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords\n",
    "\n",
    "Stopwords are those words that donot contrbute to the meaning of a sentense. Hence, they can be safely removed without causing change in the meaning of sentense. The NLTK (Natural Langugae tool kit) libarary has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens.\n",
    "\n",
    "#### For example:\n",
    "\n",
    "In Language translation we didnot remove the stopwords as they have importance over there while in the sentiment analysis you can simple remove them beacuse they have usually no impact on sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'AI', 'world', '.', 'A.I', 'besy', 'invention', 'far', '...']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus is the dataset \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#remove stopwords (stopwprds are specific to langugae wise, different for different languagaes )\n",
    "\n",
    "def rem_stopwprds(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #there is words tokenization, sentense tokenization, in words we are splitting words into list\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "text_sample = \"Data is new to AI world. A.I is the besy invention so far...\"\n",
    "rem_stopwprds(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "The Stemming is kind of process where it will be able to convert word into it's base word. In some of  the problems same word is used in different form of verb so in order to prevent model from confusion we use stemming process that is converting into it's base word.\n",
    "\n",
    "#### For Example:\n",
    "\n",
    "Mangoes -> Mango \n",
    "Boys    -> Boy\n",
    "going   -> go \n",
    "\n",
    "#### Note:\n",
    "\n",
    "if sentenses are not in form of tokens we first convert them into  tokens, After converting them into tokens then we can convert them into their root form if applicable.\n",
    "\n",
    "There are three type of stemmer:\n",
    "\n",
    "1) Porter Stemmer \n",
    "\n",
    "2) Snowball Stemmer \n",
    "\n",
    "3) Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data is the most import aspect in the field of artfici intellig . It can be either in form imag , video or audio 's .\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Port Stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem1 = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stem1.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Data is the most import aspect in the field of Artficial Intelligence. It can be either in form  Images, Videos or Audio's.\"\n",
    "\" \".join(stem_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dat is the most import aspect in the field of artf intellig . it can be eith in form im , video or audio 's .\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lancaster Stemmer\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem1 = LancasterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stem1.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Data is the most import aspect in the field of Artficial Intelligence. It can be either in form  Images, Videos or Audio's.\"\n",
    "\" \".join(stem_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The process of converting word into its nearest possible word which can have similar kind of meaning. sometimes it work like stemming. One more thing in Lemmatization is that it ensures that root word belongs to languagae.It can also find the synonums. Moreover, we can do can do lemmatization for specific things in parts of speech like over here we only did for verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Data be the most import aspect in the field of Artficial Intelligence . It can be either in form Images , Videos or Audio 's .\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma  = wordnet.WordNetLemmatizer()\n",
    "#dictionary of synonums words for mapping\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Lemmatize String\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemma.lemmatize(word ,pos='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "text = \"Data is the most import aspect in the field of Artficial Intelligence. It can be either in form  Images, Videos or Audio's.\"\n",
    "\" \".join(lemmatize_word(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "Assining tags to each word in a setense\n",
    "\n",
    "\n",
    "Just search for POS nltk tags on google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('punished', 'VBN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#convert text into words with their tags \n",
    "def pos_tagg(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "\n",
    "pos_tagg(\"Are you afraid of being punished?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We can get that tags meanining here by downloading Penn Treebank tagset \n",
    "nltk.download('tagsets')\n",
    "\n",
    "nltk.help.upenn_tagset('PRP')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.help.upenn_tagset('IN')\n",
    "nltk.help.upenn_tagset('VBG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "The process of extracting the phrases from unstructred text and give them structure. we also called them shallow parsing. we can do it on top of pos tagging. It group words into chunks for noun pharses. It usually performed with the help of regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ boy/NN)\n",
      "  is/VBZ\n",
      "  playing/VBG\n",
      "  in/IN\n",
      "  (NP the/DT park/NN))\n",
      "(NP the/DT little/JJ boy/NN)\n",
      "(NP the/DT park/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize  import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def chunking(text,grammer):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    #Labelling words with tokenize\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    #creating a chunk parser using grammer \n",
    "    chunkParser  = nltk.RegexpParser(grammer)\n",
    "    \n",
    "    #test it on the list of word tokens with tagges pos \n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    \n",
    "    tree.draw()\n",
    "senetence = 'the little boy is playing in the park' \n",
    "grammer = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(senetence,grammer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Yasir/NNP)\n",
      "  (PERSON Shah/NNP)\n",
      "  score/VBD\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  between/IN\n",
      "  (GPE Pakistan/NNP)\n",
      "  and/CC\n",
      "  (ORGANIZATION WestIndies/NNS))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\AbdulRehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk \n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def ner(text):\n",
    "    \n",
    "    #tokenize the text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    #pos tagging of words \n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    #tree of word entities\n",
    "    print(ne_chunk(word_pos))\n",
    "\n",
    "text = 'Yasir Shah score highest 400 runs in test match which played between Pakistan and WestIndies'\n",
    "ner(text)\n",
    "\n",
    "#Here these libs fails where the libs predcting the westindies country as an organization and many more cases happens even with capitalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Regex\n",
    "\n",
    "Formatting the data in our desire format\n",
    "\n",
    "\n",
    "> ^ Start of the string\n",
    "\n",
    "> w+ is used to match the alpha numeric characters in the string  and underscore too. we have use this expression wiht the \"\\\\\" slash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coummity'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "#Getting only alpha numerice characters\n",
    "\n",
    "def find_alphanumeric(text):\n",
    "    data = re.findall(r'^\\w+',text)\n",
    "    return \" \".join(data)\n",
    "\n",
    "sample_text = \"Coummity_23 helps people to grow like fast13\"\n",
    "find_alphanumeric(sample_text)\n",
    "\n",
    "# Here only the first word is coming beacuse we have a check that if the string starts with alphanumeric character or starts \n",
    "# with underscore then diaplay that work only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Online Websites\n",
    " \n",
    "> https://regex101.com/\n",
    "\n",
    "> https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some', 'String', 'comes', 'here']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the dataset based on space (Make sure youo must use \\ before s to split the string )\n",
    "\n",
    "import re \n",
    "\n",
    "re.split(r\"\\s\",\"Some String comes here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Important Methods\n",
    "\n",
    "1. re.match()\n",
    "2. re.find()\n",
    "3. re.search()\n",
    "4. re.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('icecreams', 'ice', 'ice')\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# here the regular expressions states that check if the string had i in the first and then rest of the charcters are alphanumeric\n",
    "# if yes then check \\W(that means all the charcters that are not in \\w) like space, exclamaition mark if they comes then check \n",
    "# the third group, if all ok then print that string\n",
    "\n",
    "lists= ['icecreams ice','mangoes','banana']\n",
    "\n",
    "for i in lists:\n",
    "    q = re.match(\"(i\\w+)\\W((i\\w+))\",i)\n",
    "    if q :\n",
    "        print(q.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some-email123.@gmail.com', 'emailali@gmail.com', 'hotmail1234@hotmail.ai']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "#This regular expression is weak one if there comes some text that contains @ sign so it will be going to fail so for that you can use specific gmail or hotmail \n",
    "emails = \"some-email123.@gmail.com,emailali@gmail.com,hotmail1234@hotmail.ai,abdul rehman\"\n",
    "\n",
    "re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\",emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Whenever we have kind of noisy text like 2morrow(tomarrow),2mrrw(tommorrow),tomrw(tomarrow),b4(before),otw(on the way),:)(smile) you can use text normalized from the raw text. This is the part of data preprocessing when you have to deal with the nlp problem.\n",
    "\n",
    "Text normalization is too useful for noisy text such as social media comments, comment, blos posts, abbreviations, missed spellings. Text normalization is also useful for topic extraction where near synonyms and spelling differences are common like e.g. ('topic modelling','topic modeling','topic-modelling')\n",
    "\n",
    "Some of the common approches to text normalization includes dictionary mappings, statistical machine translation(SMT),\n",
    "and spelling correction based appraoches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "text = \"There are some string that comes into the picture\"\n",
    "tokens = WhitespaceTokenizer().tokenize(text)\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WorkPunctuation Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "text = \"There is no need to panic, We need to work together, take small yet imprtant measures.\"\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'no',\n",
       " 'need',\n",
       " 'to',\n",
       " 'panic',\n",
       " ',',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'work',\n",
       " 'together',\n",
       " ',',\n",
       " 'take',\n",
       " 'small',\n",
       " 'yet',\n",
       " 'imprtant',\n",
       " 'measures',\n",
       " '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Base on the punctuations we need to tokenize so we used that punctuations tokenize\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frquency Distribution\n",
    "\n",
    "Counting your words in your texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 19 samples and 22 outcomes>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"There is a need  of a person here. if he is willing to donate somehting it will be his kindness.\"\n",
    "frequence_dist = nltk.FreqDist(word_tokenize(text))\n",
    "print(frequence_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('There', 1), ('is', 2), ('a', 2), ('need', 1), ('of', 1), ('person', 1), ('here', 1), ('.', 2), ('if', 1), ('he', 1), ('willing', 1), ('to', 1), ('donate', 1), ('somehting', 1), ('it', 1), ('will', 1), ('be', 1), ('his', 1), ('kindness', 1)])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequence_dist.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The word that is comming less time there is less chance that it will contribute in the model building...\n",
    "frequence_dist.plot(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
